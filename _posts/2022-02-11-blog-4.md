---
layout: post
title: Blog Post 4
---

In this blog post, I will walk through the process of creating a spectral clustering algorithm.

2022-02-11

First, let's demonstrate what clustering looks like. We use the `make_blobs` function create two blobs and we can plot the blobs to visualize them.


```python
import numpy as np
from sklearn import metrics
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

    
![output_2_1.png](/images/output_2_1.png) 
    


K-means can be used to separate data into "blobs" and we can control the color to see what `KMeans` predicts which blobs each data point belongs to.


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![output_4_1.png](/images/output_4_1.png)
    


That was easy, the colors indicate the blobs correctly. Let's see what happens when we use a more difficult shape.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

    
![output_6_1.png](/images/output_6_1.png)
    


K-means no longer accurately distinguishes the two clusters in this moon shape.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![output_8_1.png](/images/output_8_1.png)
    


## Part A

In order to start the process of creating a function to correctly categorize the blobs, we start with defining a similarity matrix $$A$$. 

$$A$$ is a matrix of size `(n,n)` where its entries `A[i,j]` are 1 if the points `X[i]` and `X[j]` are "close", otherwise the entry is 0. Here, we determine the points are "close" if their distance is within epsilon. We take epsilon to be 0.4.


```python
epsilon = 0.4
```

First, we construct $$A$$ using the `pairwise_distances` function to calculate the distances between the `n` data points.


```python
A = metrics.pairwise_distances(X=X)
A
```




    array([[0.        , 1.27292462, 1.33315598, ..., 1.9812102 , 1.68337039,
            1.94073324],
           [1.27292462, 0.        , 1.46325112, ..., 1.93729167, 1.68543003,
            1.91287315],
           [1.33315598, 1.46325112, 0.        , ..., 0.64857172, 0.35035968,
            0.60860868],
           ...,
           [1.9812102 , 1.93729167, 0.64857172, ..., 0.        , 0.30070415,
            0.04219636],
           [1.68337039, 1.68543003, 0.35035968, ..., 0.30070415, 0.        ,
            0.26255757],
           [1.94073324, 1.91287315, 0.60860868, ..., 0.04219636, 0.26255757,
            0.        ]])



Again, to determine whether the distances are close, we compare them to epsilon.


```python
A = (A <= epsilon)
A
```




    array([[ True, False, False, ..., False, False, False],
           [False,  True, False, ..., False, False, False],
           [False, False,  True, ..., False,  True, False],
           ...,
           [False, False, False, ...,  True,  True,  True],
           [False, False,  True, ...,  True,  True,  True],
           [False, False, False, ...,  True,  True,  True]])



Now, we convert the boolean values to numerical values of 0 and 1.


```python
A = 1.0*A
A
```




    array([[1., 0., 0., ..., 0., 0., 0.],
           [0., 1., 0., ..., 0., 0., 0.],
           [0., 0., 1., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 1., 1., 1.],
           [0., 0., 1., ..., 1., 1., 1.],
           [0., 0., 0., ..., 1., 1., 1.]])



Since the diagonal values of $$A$$ measure the distance of the point from itself, this is not what we are looking for, so we can assign the value to 0.


```python
np.fill_diagonal(a=A, val=0)
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



## Part B

The next step is to calculate the binary norm cut objective of matrix $$A$$:

$$N_A$$($$C_0$$,$$C_1$$)$$\equiv$$ $$cut$$($$C_0$$,$$C_1$$)($$\frac{1}{vol(C_0)}$$ + $$\frac{1}{vol(C_1)}$$)

### B.1 The cut term

$$cut$$($$C_0$$,$$C_1$$) counts the number of entries in $$A$$ that label points from the separate clusters as "close". The `cut(A,y)` function computes the total of these entries since closeness is indicated with the entry of 1. This function should return a small number since the data points from separate clusters should not be close.

Here `y` is the array of labels for the clusters $$C_0$$ and $$C_1$$. Since `y` contains the correct labels of the `n` data points, we compare each of the entries and add up the respective entries of `A[i][j]`. If the data points have different entries for y, their respective `A[i][j]` should be 0 (not close).


```python
def cut(A,y):
    count = 0
    for i in range(y.size):
        for j in range(i, n):
            if y[i] != y[j]:
                count += A[i][j]
    return count
```

We should check the cut term for our clusters.


```python
cut(A,y)
```




    13.0



13 is a small number considering `(n,n)` equals `(200,200)`. Great work!

Now we compute the cut term for a random vector `z` that is not related to our clusters.


```python
z = np.random.randint(2, size=n)
cut(A,z)
```




    1150.0



We observe that the cut term is much higher, which is promising for the work we have done so far. We are closer to predicting clusters.

### B.2 The volume term

The volume term should hold two values, calculating the respective volumes of clusters $$C_0$$ and $$C_1$$.

We want to find that:

1. There are relatively few entries in A that indicate points in $$C_0$$ are close to points in $$C_1$$.
2. The volumes of $$C_0$$ and $$C_1$$ are not too small.

We can use the dot product `y@A` to find the volume of $$C_1$$ since the entries of`y` are 1 for $$C_1$$ and 0 for $$C_0$$, adding up the respective values of $$A$$. Then, we subtract its volume from the total volume to get the volume of $$C_0$$.


```python
def vols(A,y):
    vol_1 = sum(y@A) 
    total_vol = sum(sum(A))
    vol_0 = total_vol - vol_1
    return (vol_0, vol_1)
```


```python
vols(A,y)
```




    (2299.0, 2217.0)



Our function works and produces the two values that we want, so we move on to formulating the binary norm cut objective as the `normcut` function.


```python
def normcut(A,y):
    vol_0, vol_1 = vols(A,y)
    return cut(A,y)*(1/vol_0 + 1/vol_1)
```


```python
normcut(A,y)
```




    0.011518412331615225



## Part C

While explicitly solving for the `normcut` works, we need a math trick to for the optimization problem.

1. We write the `transform(A,y)` function to compute the `z` vector, defined as:

    $$\frac{1}{vol(C_0)}$$ if $$y_i$$ = 0 and $$-\frac{1}{vol(C_1)}$$ if $$y_i$$ = 1


```python
def transform(A,y):
    vol_0, vol_1 = vols(A,y)
    z = y*(-1/vol_1 - 1/vol_0) + (1/vol_0)
    return z
```


```python
z = transform(A,y)
z[:5]
```




    array([-0.00045106, -0.00045106,  0.00043497,  0.00043497,  0.00043497])



The first 5 entries of our `z` vector look promising. Let's move on.

2. Now, we do some linear algebra to calculate the binary norm cut objective. We define the matrix $$D$$ to be a diagonal matrix with the entries as the degree or row-sum of $$A$$.


```python
D = np.ndarray(shape=(n,n))
np.fill_diagonal(a=D, val=sum(A))
D
```




    array([[15.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0., 25.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0., 24., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ..., 26.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0., 28.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0., 26.]])



Now we can compute `normcut` using the linear algebraic method:

$$N_A$$($$C_0$$,$$C_1$$) = $$\frac{z_T(D-A)z}{z_TDz}$$ 


```python
N = (np.transpose(z) @ (D-A) @ z)/(np.transpose(z) @ D @ z)
N
```




    0.011518412331615126




```python
np.isclose(normcut(A,y), N)
```




    True



We get that the linear algebra formula gives the same result as the `normcut(A,y)` function above.

3. Check the identity $$z_TD\mathbb{1}$$ = 0


```python
np.isclose(np.transpose(z) @ D @ np.ones(n), 0)
```




    True



## Part D

The `orth_obj` function below solves the minimization problem above with the orthogonal complement of z.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Now, the `minimize` function helps us find the minimizing vector that we name `z_min`.


```python
from scipy import optimize
min_fcn = optimize.minimize(fun=orth_obj,x0=e)
z_min = min_fcn.x
```

## Part E

We use the sign of `z_min` to distinguish the colors of the moons, helping us visualizing how we are distinguishing between the two clusters.


```python
plt.scatter(X[z_min<0][:,0], X[z_min<0][:,1], c = 'red')
plt.scatter(X[z_min>=0][:,0], X[z_min>=0][:,1], c = 'blue')
```

    
![output_50_1.png](/images/output_50_1.png)
    


This is almost perfect! The two moons are two different colors for the most part.

## Part F

Continuing with using linear algebra, we define $$L$$, the Laplacian matrix of $$A$$ and use it to find the eigenvector corresponding to its second smallest eigenvalue `z_eig`. Then, we can try plotting using the sign of `z_eig` to select the colors.


```python
L = np.linalg.inv(D)@(D-A)
eig_values = np.linalg.eig(L)[0]
ordered_ev = np.sort(eig_values)
index, = np.where(eig_values==ordered_ev[1])
index[0]
```




    1


```python
z_eig = np.linalg.eig(L)[1][:,index[0]]
plt.scatter(X[z_eig<0][:,0], X[z_eig<0][:,1], c = 'red')
plt.scatter(X[z_eig>=0][:,0], X[z_eig>=0][:,1], c = 'blue')
```

    
![output_54_1.png](/images/output_54_1.png)
    


This looks proportional to the previous graph. Awesome!

## Part G

Finally, let's write a `spectral_clustering` function that takes in the input data `X` and an `epsilon` distance parameter and returns an array of 0 and 1 to distinguish between the two clusters. It uses each of the steps that we have computed above to come to forming our array of "predictions".


```python
def spectral_clustering(X, epsilon): 
    A = metrics.pairwise_distances(X=X)
    A = (A <= epsilon)*1.0
    np.fill_diagonal(a=A, val=0)
    D = np.ndarray(shape=(n,n))
    np.fill_diagonal(a=D, val=sum(A))
    L = np.linalg.inv(D)@(D-A)
    eig_values = np.linalg.eig(L)[0]
    index, = np.where(eig_values==np.sort(eig_values)[1])
    z_eig = np.linalg.eig(L)[1][:,index[0]]
    return np.array([1 if z_eig[i]<0 else 0 for i in range(z_eig.size)])
```

## Part H

Let's test out our `spectral_clustering` function on a larger data set.


```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

    
![png](output_59_1.png)
    



```python
z = spectral_clustering(X, 0.4)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_60_1.png)
    


Using our vector `z` to determine the colors, we see that our function is not completely accurate. We can increase the noise and see how we do.


```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.10, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

    
![png](output_62_1.png)
    



```python
z = spectral_clustering(X, 0.4)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_63_1.png)
    


Not sure if this is better, but we can try again.


```python
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

    
![png](output_65_1.png)
    



```python
z = spectral_clustering(X, 0.4)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_66_1.png)
    


## Part I

Now we try out this function on a different data set. Let's use the bull's eye shape.


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

    
![png](output_68_1.png)
    


Let's see how K-means does at predicting these concentric circles.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![png](output_70_1.png)
    


That's not right, but let's see how our function does.


```python
z = spectral_clustering(X, 0.4)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_72_1.png)
    


Yup, not sure what I was expecting, but let's experiment with different `epsilon` values.


```python
z = spectral_clustering(X, 0.1)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_74_1.png)
    



```python
z = spectral_clustering(X, 1.0)
plt.scatter(X[:,0], X[:,1], c = z)
```

    
![png](output_75_1.png)
    


