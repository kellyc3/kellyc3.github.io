---
layout: post
title: Blog Post 5
---

In this week's blog post, we use `TensorFlow` to train a model to label images of cats and dogs.

2022-02-25

## $$\S$$ 1. Load Packages and Obtain Data

We run the following blocks of code to load packages and then obtain the TensorFlow dataset consisting of images of cats and dogs.


```python
import os
import tensorflow as tf
from tensorflow.keras import utils, layers
```


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 0s 0us/step
    68616192/68606236 [==============================] - 0s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.



```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Let's take a look at the tensorflow dataset.


```python
train_dataset.take(1)
```




    <TakeDataset element_spec=(TensorSpec(shape=(None, 160, 160, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>



We want to make a visualization that displays two rows of images with the first row consisting of three cats and the second row consisting of three dogs.


```python
from matplotlib import pyplot as plt 

# create a corresponding list for the image labels
for images, labels in train_dataset.take(1):
  titles = ["cat" if labels[i].numpy()==0 else "dog" for i in range(labels.shape[0])]

n_cat, n_dog = 0, 0
plt.figure(figsize=(10, 10))
# six images
for i in range(6): 
  # iterate through the train dataset
  for j in range(labels.shape[0]):
    # find the first three cats
    if (titles[j] == "cat") and (n_cat < 3): 
      ax = plt.subplot(3, 3, n_cat + 1)
      plt.imshow(images[j].numpy().astype("uint8"))
      plt.title(titles[j])
      plt.axis("off")
      n_cat += 1
    # and the first three dogs
    elif (titles[j] == "dog") and (n_dog < 3):
      ax = plt.subplot(3, 3, 3 + n_dog + 1)
      plt.imshow(images[j].numpy().astype("uint8"))
      plt.title(titles[j])
      plt.axis("off")
      n_dog += 1
```

![output_7_0.png](/images/output_7_0.png)


Now, we will create an iterator to count the number of dog labels and cat labels. We use the fact that dogs are labeled `1` while cats are `0`.


```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
labels_iterator
```




    <tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x7fd336849ed0>




```python
d_count, c_count = 0, 0
for label in labels_iterator:
    if label == 1:
      d_count += 1
    else:
      c_count += 1
```


```python
d_count, c_count
```




    (1000, 1000)



## $$\S$$ 2. First Model

We will create a `tf.keras.Sequential` model using different layers.


```python
model1 = tf.keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(rate=0.2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(2) 
])
```


```python
model1.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d_12 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_8 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     conv2d_13 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_9 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     dropout_5 (Dropout)         (None, 38, 38, 32)        0         
                                                                     
     conv2d_14 (Conv2D)          (None, 36, 36, 64)        18496     
                                                                     
     flatten_4 (Flatten)         (None, 82944)             0         
                                                                     
     dense_4 (Dense)             (None, 2)                 165890    
                                                                     
    =================================================================
    Total params: 194,530
    Trainable params: 194,530
    Non-trainable params: 0
    _________________________________________________________________



```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.2042 - accuracy: 0.9115 - val_loss: 2.9742 - val_accuracy: 0.5681
    Epoch 2/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1418 - accuracy: 0.9395 - val_loss: 2.5154 - val_accuracy: 0.5829
    Epoch 3/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1243 - accuracy: 0.9440 - val_loss: 4.0239 - val_accuracy: 0.6114
    Epoch 4/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.1129 - accuracy: 0.9545 - val_loss: 3.8673 - val_accuracy: 0.5941
    Epoch 5/20
    63/63 [==============================] - 5s 72ms/step - loss: 0.1679 - accuracy: 0.9470 - val_loss: 4.1086 - val_accuracy: 0.5644
    Epoch 6/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1551 - accuracy: 0.9470 - val_loss: 4.4888 - val_accuracy: 0.5792
    Epoch 7/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.1387 - accuracy: 0.9590 - val_loss: 4.2773 - val_accuracy: 0.5866
    Epoch 8/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1524 - accuracy: 0.9440 - val_loss: 3.3880 - val_accuracy: 0.5743
    Epoch 9/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1584 - accuracy: 0.9575 - val_loss: 3.3387 - val_accuracy: 0.5965
    Epoch 10/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.1429 - accuracy: 0.9595 - val_loss: 4.1614 - val_accuracy: 0.5854
    Epoch 11/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1132 - accuracy: 0.9640 - val_loss: 3.8497 - val_accuracy: 0.5743
    Epoch 12/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.1122 - accuracy: 0.9610 - val_loss: 4.4208 - val_accuracy: 0.5804
    Epoch 13/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.0890 - accuracy: 0.9735 - val_loss: 4.5568 - val_accuracy: 0.5928
    Epoch 14/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 5.2479 - val_accuracy: 0.5916
    Epoch 15/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.0634 - accuracy: 0.9750 - val_loss: 5.3828 - val_accuracy: 0.5780
    Epoch 16/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.0950 - accuracy: 0.9770 - val_loss: 5.1932 - val_accuracy: 0.6139
    Epoch 17/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.2217 - accuracy: 0.9470 - val_loss: 4.9116 - val_accuracy: 0.5903
    Epoch 18/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1940 - accuracy: 0.9440 - val_loss: 5.0262 - val_accuracy: 0.6064
    Epoch 19/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.3607 - accuracy: 0.8720 - val_loss: 4.6199 - val_accuracy: 0.5557
    Epoch 20/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.1378 - accuracy: 0.9565 - val_loss: 4.4942 - val_accuracy: 0.5792


1. **The accuracy of my model stabilized between 55% and 60% during training.**
2. We ended up doing almost 8% better than the baseline.
3. Yes, we do observe overfitting in `model1` with the training accuracy increasing to 96% while the validation accuracy does not pass 60%.

## $$\S$$ 3. Model with Data Augmentation

We start with plotting an image and then we will use `tf.keras.layers.RandomFlip()` to create modified versions of the image to include in the training dataset.


```python
plt.figure(figsize=(5, 5))
image = []
for images, labels in train_dataset.take(1):
  image = images[0]
  plt.imshow(image.numpy().astype("uint8"))
  plt.axis("off")
  break
```


![output_18_0.png](/images/output_18_0.png)



```python
data_augmentation_1 = tf.keras.Sequential([
  tf.keras.layers.RandomFlip("horizontal_and_vertical")
])

augmented_image = data_augmentation_1(tf.expand_dims(image, 0))
plt.imshow(augmented_image[0] / 255)
plt.axis('off')
```




    (-0.5, 159.5, 159.5, -0.5)




![output_19_1.png](/images/output_19_1.png)



```python
augmented_image = data_augmentation_1(tf.expand_dims(image, 0))
plt.imshow(augmented_image[0] / 255)
plt.axis('off')
```




    (-0.5, 159.5, 159.5, -0.5)




![output_20_1.png](/images/output_20_1.png)


So we've randomly flipped the image horizontally and then vertically. We can also try rotation using `tf.keras.layers.RandomRotation()`.


```python
data_augmentation_2 = tf.keras.Sequential([
  layers.RandomRotation(0.2)
])

rotated_image_1 = data_augmentation_2(image)
plt.imshow(rotated_image_1.numpy().astype("uint8"))
plt.axis("off")
```




    (-0.5, 159.5, 159.5, -0.5)




![output_22_1.png](/images/output_22_1.png)



```python
data_augmentation_2 = tf.keras.Sequential([
  layers.RandomRotation(1)
])

rotated_image_2 = data_augmentation_2(image)
plt.imshow(rotated_image_2.numpy().astype("uint8"))
plt.axis("off")
```




    (-0.5, 159.5, 159.5, -0.5)




![output_23_1.png](/images/output_23_1.png)


We will now plot the original and the rotated images in the same plot.


```python
images = [image, rotated_image_1, rotated_image_2]
rotations = ['original', 'rotation 1', 'rotation 2']
plt.figure(figsize=(6, 6))
for i in range(3): 
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(images[i].numpy().astype("uint8"))
  plt.title(rotations[i])
  plt.axis("off")
```


![output_25_0.png](/images/output_25_0.png)


We can create and train a model for data augmentation.


```python
model2 = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical", input_shape=(160, 160, 3)),                            
  layers.RandomRotation(0.2),
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(rate=0.2),
  layers.Conv2D(64, (3, 3), activation='relu'),
  layers.Flatten(),
  layers.Dense(2) 
])
```


```python
model2.summary()
```

    Model: "sequential_32"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     random_flip_30 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_4 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     conv2d_18 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_12 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_19 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_13 (MaxPoolin  (None, 38, 38, 32)       0         
     g2D)                                                            
                                                                     
     dropout_7 (Dropout)         (None, 38, 38, 32)        0         
                                                                     
     conv2d_20 (Conv2D)          (None, 36, 36, 64)        18496     
                                                                     
     flatten_6 (Flatten)         (None, 82944)             0         
                                                                     
     dense_6 (Dense)             (None, 2)                 165890    
                                                                     
    =================================================================
    Total params: 194,530
    Trainable params: 194,530
    Non-trainable params: 0
    _________________________________________________________________



```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 5s 57ms/step - loss: 15.8111 - accuracy: 0.5120 - val_loss: 0.6965 - val_accuracy: 0.5186
    Epoch 2/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6999 - accuracy: 0.4885 - val_loss: 0.6957 - val_accuracy: 0.5223
    Epoch 3/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6970 - accuracy: 0.4975 - val_loss: 0.6947 - val_accuracy: 0.4913
    Epoch 4/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6957 - accuracy: 0.5045 - val_loss: 0.6946 - val_accuracy: 0.5087
    Epoch 5/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.6920 - accuracy: 0.5065 - val_loss: 0.6945 - val_accuracy: 0.4901
    Epoch 6/20
    63/63 [==============================] - 5s 80ms/step - loss: 0.6857 - accuracy: 0.5490 - val_loss: 0.6955 - val_accuracy: 0.5025
    Epoch 7/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.6861 - accuracy: 0.5290 - val_loss: 0.6951 - val_accuracy: 0.5309
    Epoch 8/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.6818 - accuracy: 0.5545 - val_loss: 0.6948 - val_accuracy: 0.5062
    Epoch 9/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6773 - accuracy: 0.5525 - val_loss: 0.6936 - val_accuracy: 0.5359
    Epoch 10/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.6788 - accuracy: 0.5365 - val_loss: 0.6868 - val_accuracy: 0.5285
    Epoch 11/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.6845 - accuracy: 0.5395 - val_loss: 0.6956 - val_accuracy: 0.4938
    Epoch 12/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.6685 - accuracy: 0.5610 - val_loss: 0.6922 - val_accuracy: 0.5297
    Epoch 13/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6709 - accuracy: 0.5685 - val_loss: 0.6877 - val_accuracy: 0.5371
    Epoch 14/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6812 - accuracy: 0.5585 - val_loss: 0.6996 - val_accuracy: 0.4938
    Epoch 15/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6722 - accuracy: 0.5650 - val_loss: 0.6822 - val_accuracy: 0.5507
    Epoch 16/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6703 - accuracy: 0.5685 - val_loss: 0.6862 - val_accuracy: 0.5309
    Epoch 17/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6795 - accuracy: 0.5375 - val_loss: 0.6665 - val_accuracy: 0.5755
    Epoch 18/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6874 - accuracy: 0.5205 - val_loss: 0.6938 - val_accuracy: 0.5149
    Epoch 19/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.6635 - accuracy: 0.5780 - val_loss: 0.6673 - val_accuracy: 0.5755
    Epoch 20/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6646 - accuracy: 0.5755 - val_loss: 0.6647 - val_accuracy: 0.5891


1. **The accuracy of my model stabilized between 55% and 60% during training.**
2. We ended up doing almost 9% better than the baseline and 1% better than `model1`.
3. No, we do not observe overfitting in `model2` with the training accuracy ending up slightly greater than the training accuracy.

## $$\S$$ 4. Data Preprocessing

First, we create a data preprocessing layer. Then, we use it in our next model `model3`.


```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```


```python
model3 = tf.keras.Sequential([
  preprocessor,
  layers.RandomFlip("horizontal_and_vertical"),                            
  layers.RandomRotation(0.2),
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(rate=0.2),
  layers.Conv2D(64, (3, 3), activation='relu'),
  layers.Flatten(),
  layers.Dense(2) 
])
```


```python
model3.summary()
```

    Model: "sequential_36"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_34 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_5 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     conv2d_21 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_14 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_22 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_15 (MaxPoolin  (None, 38, 38, 32)       0         
     g2D)                                                            
                                                                     
     dropout_8 (Dropout)         (None, 38, 38, 32)        0         
                                                                     
     conv2d_23 (Conv2D)          (None, 36, 36, 64)        18496     
                                                                     
     flatten_7 (Flatten)         (None, 82944)             0         
                                                                     
     dense_7 (Dense)             (None, 2)                 165890    
                                                                     
    =================================================================
    Total params: 194,530
    Trainable params: 194,530
    Non-trainable params: 0
    _________________________________________________________________



```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 5s 62ms/step - loss: 0.7442 - accuracy: 0.5300 - val_loss: 0.6628 - val_accuracy: 0.5780
    Epoch 2/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.6629 - accuracy: 0.5855 - val_loss: 0.6662 - val_accuracy: 0.6040
    Epoch 3/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.6511 - accuracy: 0.5975 - val_loss: 0.6417 - val_accuracy: 0.6262
    Epoch 4/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.6324 - accuracy: 0.6250 - val_loss: 0.6458 - val_accuracy: 0.6089
    Epoch 5/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.6139 - accuracy: 0.6575 - val_loss: 0.6194 - val_accuracy: 0.6683
    Epoch 6/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.6033 - accuracy: 0.6555 - val_loss: 0.6074 - val_accuracy: 0.6757
    Epoch 7/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.5831 - accuracy: 0.6850 - val_loss: 0.6050 - val_accuracy: 0.6906
    Epoch 8/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5745 - accuracy: 0.7040 - val_loss: 0.6143 - val_accuracy: 0.6757
    Epoch 9/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.5703 - accuracy: 0.6920 - val_loss: 0.6066 - val_accuracy: 0.6856
    Epoch 10/20
    63/63 [==============================] - 4s 64ms/step - loss: 0.5755 - accuracy: 0.7005 - val_loss: 0.5850 - val_accuracy: 0.7005
    Epoch 11/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5673 - accuracy: 0.6940 - val_loss: 0.6021 - val_accuracy: 0.6844
    Epoch 12/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5602 - accuracy: 0.7095 - val_loss: 0.5712 - val_accuracy: 0.7005
    Epoch 13/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5441 - accuracy: 0.7170 - val_loss: 0.5709 - val_accuracy: 0.7215
    Epoch 14/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.5492 - accuracy: 0.7160 - val_loss: 0.5584 - val_accuracy: 0.7277
    Epoch 15/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.5365 - accuracy: 0.7310 - val_loss: 0.5682 - val_accuracy: 0.7228
    Epoch 16/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.5328 - accuracy: 0.7285 - val_loss: 0.5445 - val_accuracy: 0.7240
    Epoch 17/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.5470 - accuracy: 0.7135 - val_loss: 0.5771 - val_accuracy: 0.6931
    Epoch 18/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.5503 - accuracy: 0.7070 - val_loss: 0.5317 - val_accuracy: 0.7364
    Epoch 19/20
    63/63 [==============================] - 4s 62ms/step - loss: 0.5359 - accuracy: 0.7415 - val_loss: 0.5417 - val_accuracy: 0.7327
    Epoch 20/20
    63/63 [==============================] - 4s 63ms/step - loss: 0.5376 - accuracy: 0.7200 - val_loss: 0.5238 - val_accuracy: 0.7302


1. **The accuracy of my model stabilized between 70% and 75% during training.**
2. We ended up doing over 23% better than the baseline and 14% better than `model2`.
3. No, we do not observe overfitting in `model3` with the validation accuracy 1% greater than the training acurracy.

## $$\S$$ 5. Transfer Learning

Now, we will attempt to use a pre-existing model to train our model. We must first download `MobileNetV2` using the following code:



```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step



```python
model4 = tf.keras.Sequential([
  preprocessor,
  #layers.GlobalMaxPooling2D(), 
  #layers.Dense(1),
  layers.RandomFlip("horizontal_and_vertical"),                            
  layers.RandomRotation(0.2),
  base_model_layer,
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dense(2) # number of classes
])
```


```python
model4.summary()
```

    Model: "sequential_7"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_7 (RandomFlip)  (None, 160, 160, 3)       0         
                                                                     
     random_rotation_6 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     dropout_4 (Dropout)         (None, 5, 5, 1280)        0         
                                                                     
     flatten_5 (Flatten)         (None, 32000)             0         
                                                                     
     dense_13 (Dense)            (None, 2)                 64002     
                                                                     
    =================================================================
    Total params: 2,321,986
    Trainable params: 64,002
    Non-trainable params: 2,257,984
    _________________________________________________________________


There are only 64,002 trainable parameters in `model4`. Look at the history to see how that affects accuracy.


```python
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 21s 133ms/step - loss: 0.5283 - accuracy: 0.8685 - val_loss: 0.0861 - val_accuracy: 0.9666
    Epoch 2/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1971 - accuracy: 0.9255 - val_loss: 0.0719 - val_accuracy: 0.9752
    Epoch 3/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1883 - accuracy: 0.9265 - val_loss: 0.0799 - val_accuracy: 0.9728
    Epoch 4/20
    63/63 [==============================] - 7s 103ms/step - loss: 0.1646 - accuracy: 0.9365 - val_loss: 0.0719 - val_accuracy: 0.9777
    Epoch 5/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1383 - accuracy: 0.9450 - val_loss: 0.0820 - val_accuracy: 0.9765
    Epoch 6/20
    63/63 [==============================] - 7s 109ms/step - loss: 0.1509 - accuracy: 0.9435 - val_loss: 0.0676 - val_accuracy: 0.9703
    Epoch 7/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.1211 - accuracy: 0.9470 - val_loss: 0.0803 - val_accuracy: 0.9715
    Epoch 8/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1314 - accuracy: 0.9450 - val_loss: 0.0742 - val_accuracy: 0.9691
    Epoch 9/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.1273 - accuracy: 0.9520 - val_loss: 0.0681 - val_accuracy: 0.9715
    Epoch 10/20
    63/63 [==============================] - 7s 108ms/step - loss: 0.1300 - accuracy: 0.9475 - val_loss: 0.0722 - val_accuracy: 0.9703
    Epoch 11/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1301 - accuracy: 0.9525 - val_loss: 0.0806 - val_accuracy: 0.9715
    Epoch 12/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.1124 - accuracy: 0.9525 - val_loss: 0.0667 - val_accuracy: 0.9752
    Epoch 13/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.1134 - accuracy: 0.9565 - val_loss: 0.0857 - val_accuracy: 0.9653
    Epoch 14/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.1113 - accuracy: 0.9570 - val_loss: 0.0706 - val_accuracy: 0.9790
    Epoch 15/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.0977 - accuracy: 0.9595 - val_loss: 0.0858 - val_accuracy: 0.9678
    Epoch 16/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.0930 - accuracy: 0.9635 - val_loss: 0.0707 - val_accuracy: 0.9777
    Epoch 17/20
    63/63 [==============================] - 7s 104ms/step - loss: 0.1182 - accuracy: 0.9580 - val_loss: 0.0836 - val_accuracy: 0.9678
    Epoch 18/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.0933 - accuracy: 0.9685 - val_loss: 0.0717 - val_accuracy: 0.9765
    Epoch 19/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.0785 - accuracy: 0.9690 - val_loss: 0.0778 - val_accuracy: 0.9728
    Epoch 20/20
    63/63 [==============================] - 7s 105ms/step - loss: 0.0831 - accuracy: 0.9730 - val_loss: 0.0980 - val_accuracy: 0.9715


1. **The accuracy of my model stabilized around 97% during training.**
2. We ended up doing 24% better than `model3`.
3. No, we do not observe overfitting in `model4` with the training accuracy and the validation accuracy roughly at the same rate.

## $$\S$$ 6. Score on Test Data

Since `model4` has the best accuracy, let's test it with the `test_dataset`.


```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 100ms/step - loss: 0.0843 - accuracy: 0.9635
    Test accuracy : 0.9635416865348816


Hooray! We achieved 96% accuracy with our model. We have successfully trained and evaluated a machine learning model.
