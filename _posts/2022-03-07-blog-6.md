---
layout: post
title: Blog Post 6
---

We will now learn to use `TensorFlow` to train three models to predict whether an article is fake news.

2022-03-07

# $$\S$$1. Acquire Training Data


```python
import pandas as pd
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df.head()
```





  <div id="df-0b16fcdc-832d-44eb-9592-609d37240939">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-0b16fcdc-832d-44eb-9592-609d37240939')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-0b16fcdc-832d-44eb-9592-609d37240939 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-0b16fcdc-832d-44eb-9592-609d37240939');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  



Each row in the dataset is an article, containing its title, text, and whether it is fake news in the respective columns. The `fake` column indicates `0` if the article is true and `1` if it is fake news.

# $$\S$$2. Make a Dataset

We will create a TensorFlow dataset from the dataframe above.


```python
import numpy as np
import tensorflow as tf
import re
import string

from tensorflow.keras import losses

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```


```python
from nltk.corpus import stopwords

def make_dataset(data_df):
  """
  Takes in the pandas dataframe and removes the stopwords from the title and 
  text of the articles and creates a TensorFlow dataset with (title, text) as 
  the input and the fake column as the output.
  """
  # normalizing the text to lowercase and separating the words
  data_df["title"] = data_df["title"].astype(str).str.lower().str.split()
  data_df["text"] = data_df["text"].astype(str).str.lower().str.split()
  
  # removing the stopwords
  stop = stopwords.words('english')
  data_df["title"] = data_df["title"].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))
  data_df["text"] = data_df["text"].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))
  
  # creating the dataset
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df[["text"]]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
  )
  data = data.batch(100) # separate dataset into 100 batches
  return data
```


```python
df.shape
```




    (22449, 4)




```python
data = make_dataset(df)
len(data)
```




    225



Now, we split the data into training and validation datasets.


```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)

len(train), len(val)
```




    (180, 45)



Find the base rate for the accuracy of a model that always guesses that the article is "fake news" by looking at the labels on the training set.


```python
train.element_spec
```




    ({'text': TensorSpec(shape=(None, 1), dtype=tf.string, name=None),
      'title': TensorSpec(shape=(None, 1), dtype=tf.string, name=None)},
     {'fake': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)})



We take the numpy array from the Tensor object and create an array of true `fake` values to compare to an array of ones (all entries guess `fake`).


```python
for input, output in train.take(1):
 fake_np = output['fake'] 

fake_arr = np.transpose(fake_np)[0]
fake_arr
```




    array([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
           0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
           0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
           1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
           1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0])




```python
all_fake = np.ones(fake_arr.size)
(all_fake == fake_arr).mean()
```




    0.51



We have a 51% base rate for a model that always guesses `1` or `fake`.


```python
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

# $$\S$$3. Create Models

We will create three TensorFlow models to determine whether the title of the article, the text of the article, or both is the most effective in detecting fake news.

Our first model will only use the article title as an input.


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# create input as article title
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)

# create input as article text
text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```


```python
# layers for processing the title
title_features = vectorize_layer(title_input)
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding1")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)

# repeat for processing the text
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

Let's create `model1` to take only the title input.


```python
# call a layer to the input object
main = layers.Dense(32, activation="relu")(title_features)

# create a layer for the output
#main = layers.Dense(32, activation="relu")(main)
output = layers.Dense(2, name = "fake")(main)

# create the model using the input and output defined above
model1 = keras.Model(inputs=title_input, outputs = output, name="mnist_model")
model1.summary()
```

    Model: "mnist_model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding1 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout_4 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_2   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_5 (Dropout)         (None, 3)                 0         
                                                                     
     dense_4 (Dense)             (None, 32)                128       
                                                                     
     dense_8 (Dense)             (None, 32)                1056      
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 7,250
    Trainable params: 7,250
    Non-trainable params: 0
    _________________________________________________________________



```python
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model1.fit(train, 
                     epochs=50, 
                     validation_data=val,
                     verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "accuracy")
plt.plot(history.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```




![output_33_1.png](/images/output_33_1.png)


The validation accuracy looks good, no signs of overfitting!

We define the next model as `model2` which only takes the article text as input.


```python
# call a layer to the input object
main2 = layers.Dense(32, activation="relu")(text_features)

# create a layer for the output
main2 = layers.Dense(32, activation="relu")(main2)
output2 = layers.Dense(2, name = "fake")(main2)

# create the model using the input and output defined above
model2 = keras.Model(inputs=text_input, outputs = output2, name="mnist_model")
model2.summary()
```

    Model: "mnist_model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding2 (Embedding)      (None, 500, 3)            6000      
                                                                     
     dropout_6 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_3   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_7 (Dropout)         (None, 3)                 0         
                                                                     
     dense_5 (Dense)             (None, 32)                128       
                                                                     
     dense_9 (Dense)             (None, 32)                1056      
                                                                     
     dense_10 (Dense)            (None, 32)                1056      
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 8,306
    Trainable params: 8,306
    Non-trainable params: 0
    _________________________________________________________________



```python
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history2 = model2.fit(train, 
                     epochs=50, 
                     validation_data=val,
                     verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)



```python
from matplotlib import pyplot as plt
plt.plot(history2.history["accuracy"], label = "accuracy")
plt.plot(history2.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```




![output_27_1.png](/images/output_27_1.png)


Our final model uses both the article's title and text as input in `model3`.


```python
main3 = layers.concatenate([title_features, text_features], axis = 1)
# call a layer to the input object
main3 = layers.Dense(32, activation="relu")(main3)

# create a layer for the output
output3 = layers.Dense(2, name = "fake")(main3)

# create the model using the input and output defined above
model3 = keras.Model(inputs=[title_input, text_input], outputs = output3, name="mnist_model")
model3.summary()
```

    Model: "mnist_model"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['title[0][0]',                  
     ization)                                                         'text[0][0]']                   
                                                                                                      
     embedding1 (Embedding)         (None, 500, 3)       6000        ['text_vectorization[2][0]']     
                                                                                                      
     embedding2 (Embedding)         (None, 500, 3)       6000        ['text_vectorization[3][0]']     
                                                                                                      
     dropout_4 (Dropout)            (None, 500, 3)       0           ['embedding1[0][0]']             
                                                                                                      
     dropout_6 (Dropout)            (None, 500, 3)       0           ['embedding2[0][0]']             
                                                                                                      
     global_average_pooling1d_2 (Gl  (None, 3)           0           ['dropout_4[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     global_average_pooling1d_3 (Gl  (None, 3)           0           ['dropout_6[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_5 (Dropout)            (None, 3)            0           ['global_average_pooling1d_2[0][0
                                                                     ]']                              
                                                                                                      
     dropout_7 (Dropout)            (None, 3)            0           ['global_average_pooling1d_3[0][0
                                                                     ]']                              
                                                                                                      
     dense_4 (Dense)                (None, 32)           128         ['dropout_5[0][0]']              
                                                                                                      
     dense_5 (Dense)                (None, 32)           128         ['dropout_7[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense_4[0][0]',                
                                                                      'dense_5[0][0]']                
                                                                                                      
     dense_11 (Dense)               (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_11[0][0]']               
                                                                                                      
    ==================================================================================================
    Total params: 14,402
    Trainable params: 14,402
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history3 = model3.fit(train, 
                     epochs=50, 
                     validation_data=val,
                     verbose = False)
```


```python
from matplotlib import pyplot as plt
plt.plot(history3.history["accuracy"], label = "accuracy")
plt.plot(history3.history["val_accuracy"], label = "val_accuracy")
plt.legend()
```



![output_31_1.png](/images/output_31_1.png)



```python
score1 = model1.evaluate(val)
print("model1 accuracy:", score1)

score2 = model2.evaluate(val)
print("model2 accuracy:", score2)

score3 = model3.evaluate(val)
print("model3 accuracy:", score3)
```

    45/45 [==============================] - 1s 8ms/step - loss: 0.2192 - accuracy: 0.9178
    model1 accuracy: [0.21924689412117004, 0.9177777767181396]
    45/45 [==============================] - 1s 15ms/step - loss: 0.0221 - accuracy: 0.9940
    model2 accuracy: [0.022104185074567795, 0.9940000176429749]
    45/45 [==============================] - 1s 21ms/step - loss: 7.3872e-04 - accuracy: 1.0000
    model3 accuracy: [0.000738724775146693, 1.0]


As we saw from the accuracy plots, `model3` has the highest score.

# $$\S$$4. Model Evaluation

Here we will take our best model and see how it performs against the test data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_df.head()
```





  <div id="df-151e67c7-56cc-41c8-8dc3-43ce90f3a15a">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420</td>
      <td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
      <td>Donald Trump practically does something to cri...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14902</td>
      <td>Exclusive: Kremlin tells companies to deliver ...</td>
      <td>The Kremlin wants good news.  The Russian lead...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>322</td>
      <td>Golden State Warriors Coach Just WRECKED Trum...</td>
      <td>On Saturday, the man we re forced to call  Pre...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16108</td>
      <td>Putin opens monument to Stalin's victims, diss...</td>
      <td>President Vladimir Putin inaugurated a monumen...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10304</td>
      <td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
      <td>Apparently breaking the law and scamming the g...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-151e67c7-56cc-41c8-8dc3-43ce90f3a15a')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-151e67c7-56cc-41c8-8dc3-43ce90f3a15a button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-151e67c7-56cc-41c8-8dc3-43ce90f3a15a');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  



Convert dataframe to a dataset by using our `make_dataset` function. Then, use it to evaluate our model.


```python
test_data = make_dataset(test_df)
loss, accuracy = model3.evaluate(test_data)
print('Test accuracy :', accuracy)
```

    225/225 [==============================] - 2s 11ms/step - loss: 0.0011 - accuracy: 0.9998
    Test accuracy : 0.9997772574424744


Congrats we have reached 99.98% accuracy for detecting whether an article is fake news!

# $$\S$$5. Embedding Visualization

Let's taking a look at what just happened with the embedding of our model.


```python
weights = model3.get_layer('embedding1').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word'    : vocab, 
    'not fake': weights[:,0],
    'fake'    : weights[:,1]
})
```


```python
fig = px.scatter(embedding_df, 
                 x = "not fake", 
                 y = "fake", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 6,
                 hover_name = "word")

fig.show()
```




`Myanmar`, `Ukraine`, and `Rohingya` are proper nouns that appear on the left side of the graph, being more associated with not being fake news. The words `absolutely` and `actually` are found on the upperleft edges of the plot, being more associated with fake news.

